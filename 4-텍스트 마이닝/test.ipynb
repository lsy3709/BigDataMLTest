{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝\n",
    "### 감성 분석, 토픽 분석 \n",
    "### 샘플 데이터, 네이버 크롤러 수집된 , 뉴스 기사, 영화 리뷰 데이터 등. \n",
    "### \n",
    "### 용어정리 \n",
    "### 텍스트 마이닝 \n",
    "### 비정형 텍스트 데이터에서 패턴을 찾아내고 의미 있는 정보를 추출 및 분석 과정. \n",
    "### 과정 : 텍스트 전처리 -> 특성 벡터화 -> 머신 러닝 모델 구축 및 학습 / 평가  -> 시각화 등. \n",
    "\n",
    "### 특성 벡터화와 특성 추출 \n",
    "### 단어 기반의 특성 추출을 하고, 숫자 형 값인 벡터 값으로 표현. \n",
    "### 특성 벡터화 종류 -> 1) BoW(Bag of Words)= 말뭉치 와 Word2vec \n",
    "### 1) BoW -> 문서가 가지고 있는 모든 단어에 대해 순서는 무시하고, 빈도만 고려해서 \n",
    "### 해당 단어가 얼마나 자주 등장하는지 를 특성 벡터를 만드는 방법. \n",
    "### 종류) a) 카운트 기반 벡터 b) TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터 방식 \n",
    "\n",
    "### a) 카운트 기반 벡터\n",
    "### 단어 피처에 숫자형 값을 할당 할시, 각 문서에서 해당 단어가 등장하는 횟수, \n",
    "### 단어의 빈도를 부여하는 벡터화 방식(숫자형으로 변경한다. )\n",
    "### 예) 전체 문서에 등장한 단어를 기반으로 어휘 사전을 생성하고, 단어마다 등장 횟수를 카운트하여 \n",
    "### 해당 단어의 정수 벡터 값으로 할당한다.\n",
    "### 문서별 단어의 빈도를 정리하는 , 행렬 -> 문서 단어 행렬 (DTM=Document Term Matrix)\n",
    "### 단어 출현 빈도가 높을수록 중요한 단어로 취급한다. \n",
    "### 문서 d에 등장하는 단어 t 의 횟수 -> tf(t,d) 표기 \n",
    "### 단어 행렬 : 예) 문서1 -> 사과: 10 , 점심 : 30 , -> tf(\"점심\", 문서1) = 30\n",
    "### 사이킷런에서 -> CountVectorizer 모듈(파일)에서 제공함. \n",
    "\n",
    "\n",
    "### b) TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터 방식\n",
    "### 카운트 기반에서, 단순 해당 단어빈도가 많으면 그냥 중요하다고 단순 판단했음. \n",
    "### 해당 단어가 단지 문장 구성상 많이 사용하는 단어 일 가능성 일수도 있다. \n",
    "### TF-IDF 는 \n",
    "### 특정 문서에 많이 나타나는 단어는 해당 문서의 단어 벡터에 가중치를 높이고 \n",
    "### 모든 문서에 많이 나타나는 단어는 문서의 특징을 나타내는 단어가 아니고, 그냥 범용적으로 사용이 되는 단어 가중치를 낮추는 방식. \n",
    "### 문서 d에 등장한 단어 t의 TF-IDF 표현식 \n",
    "### tf - idf(t,d) = tf(t,d) x idf(t,d)\n",
    "### tf(t,d) : 문서 d에 등장하는 단어 t 의 횟수\n",
    "### idf(t,d) : 역문서 빈도 \n",
    "### idf(t,d) = log (nd)/ 1+ df(t,d)\n",
    "\n",
    "### 집계 형식 : 문서1 : 사과 : 0.2 , 점심 : 0.6, -> tf-idf(\"점심\", 문서1) = 0.6\n",
    "\n",
    "### 사이킷런에서 TfidfVectorizer 모듈에서 제공이 됨. \n",
    "\n",
    "### 감성분석 \n",
    "### 텍스트에서 사용자의 주관적인 의견, 감성, 태도를 분석하는 텍스트 마이닝 핵심 분석 기법 중 하나, \n",
    "### 오피니언 마이닝이라고도 부름. \n",
    "### 감성을 나타내는 단어를 기반으로 긍정 또는 부정인지 결정을 함. \n",
    "### 감성 사전 기반의 감성 분석을 함. \n",
    "### 해당 감성 사전을 가진 상태에서 단어를 검색해서 점수를 계산함. \n",
    "\n",
    "### 토픽 모델링 \n",
    "### 문서를 구성하는 키워드를 기반으로 토픽(주제)를 추출하고, 추출한 토픽을 기준으로 문서를 분류\n",
    "### 분류(클러스터링) 및 분석. 토픽을 도출하고, 동향을 파악해서, 새로운 문서의 토픽을 예측하는 기법(분석)\n",
    "\n",
    "### 토픽 모델링을 하는 분석 방법 중 하나 -> LDA(Latent Dirichlet Allocation)\n",
    "### 주어진 문서에 잠재되어 있는 토픽을 추론하는 확률 모델 알고리즘. \n",
    "### 하나의 문서에 여러 토픽으로 구성되어 있고, 문서의 토픽의 분포에 따라서 단어의 분포가 결정됨. \n",
    "### 하나의 문서에 잠재된 토픽별 단어와 관련된 토픽별 비율을 도출하는 방법. \n",
    "### 프로세스 \n",
    "### 디리클레 매개변수 -> 문서 d의 토픽비율 -> 문서 d의 단어 n에서 토픽할당. -> 문서 d의 단어 n \n",
    "### 토픽 하이퍼 매개변수 -> 토픽 k의 단어 분포 생성 확률 -> 문서 d의 단어 n\n",
    "\n",
    "### pyLDAvis , 시각화 도구 \n",
    "### 유사성에 따라 토픽간 거리 지도와 선택한 토픽에 관련성 높은 단어 30개를 바차트로 시각화 해주는 도구. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
