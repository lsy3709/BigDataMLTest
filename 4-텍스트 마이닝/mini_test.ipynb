{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 기본 도구 가져오기\n",
    "import pickle\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "def okt_tokenizer(text):\n",
    "  tokens = okt.morphs(text)\n",
    "  return tokens"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#학습된 모델 가져오기, tfidf, 영화평점, 약 학습,평가요 총\n",
    "# 약 20만개 데이터를 기준으로 학습된 모델 불러오기.\n",
    "tfidf_model_save_path = \"./tfidf_model.pkl\"\n",
    "with open(tfidf_model_save_path, \"rb\") as file:\n",
    "    tfidf = pickle.load(file)\n",
    "\n",
    "print(\"✅ TF-IDF 모델 불러오기 완료\")"
   ],
   "id": "4c0560ddddfd9a92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "model_save_path = \"./SA_lr_best.pkl\"\n",
    "with open(model_save_path, \"rb\") as file:\n",
    "    SA_lr_best = pickle.load(file)\n",
    "\n",
    "print(\"✅ 모델 불러오기 완료\")\n"
   ],
   "id": "3263d452d9803494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 학습된 모델을 평가 했더니, 정확도 약 85% 정도 일치\n",
    "# 긍정, 부정을 분석 일치 여부.\n",
    "# 임의로, 특정의 문장을 , 이 모델에 입력시키면, 그 결과가, 긍정?, 부정? 판단을 해줌.\n",
    "# 정확도 약85% 이지만, 생각보다 성능이 많이 떨어짐. 참고하고\n",
    "st = input(\"감성 분석하기위한 문장을 입력 해주세요: \")\n"
   ],
   "id": "838e3224c4ad79b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "st = re.compile(r\"[ㄱ - | 가-힣]+\").findall(st)\n",
    "print(st)\n",
    "st = [\" \".join(st)]\n",
    "print(st)\n"
   ],
   "id": "f7dfc9e0f554c596"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 해당 문장을 분석하기 좋게 벡터화 과정.\n",
    "st_tfidf = tfidf.transform(st)"
   ],
   "id": "bd6f4dd002d43185"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 만들어둔 모델을 이용해서, 예측값 구하기(평가)\n",
    "st_predict = SA_lr_best.predict(st_tfidf)"
   ],
   "id": "6402a4aa775c348f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 긍정 인지, 부정인지 판별하기.\n",
    "if(st_predict == 0):\n",
    "  print(st,\" -> 부정\")\n",
    "else:\n",
    "  print(st,\" -> 긍정\")"
   ],
   "id": "b00c54a8cf3fc7b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 코로나 뉴스 json -> 메모리에 불러오기.\n",
    "import json\n",
    "file_name = '트럼프_naver_news'\n",
    "with open(\"./\"+file_name+'.json',encoding='utf-8') as j_f:\n",
    "  data = json.load(j_f)\n",
    "print(data)"
   ],
   "id": "3703d0c79d2cf5be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 해당 기사의 제목, 내용에 대해서,\n",
    "# 기본적인 전처리 작업.\n",
    "# 제목, 내용을 담을 임시 리스트 만들기.\n",
    "data_title = []\n",
    "data_description = []\n",
    "\n",
    "for item in data:\n",
    "  data_title.append(item[\"title\"])\n",
    "  data_description.append(item[\"description\"])\n",
    "\n",
    "# data_title\n",
    "data_description"
   ],
   "id": "e4fda9530777d9ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 리스트 -> 데이터 프레임 (표) 형태로 변환\n",
    "import pandas as pd\n",
    "data_df = pd.DataFrame({\"title\":data_title, \"description\" : data_description})\n",
    "data_df.head()"
   ],
   "id": "a2aaf989c270fffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 전처리, 1)제목 2)내용 한글만 남기기 작업.\n",
    "# re (regex 정규식) 이용해서, 한글만 남기기.\n",
    "import re\n",
    "# 제목에서 한글만 추출\n",
    "data_df[\"title\"]= data_df[\"title\"].apply(lambda x: re.sub(r'[^ ㄱ - | 가-힣]+',\" \",x))\n",
    "# 내용에서 한글만 추출\n",
    "data_df[\"description\"]= data_df[\"description\"].apply(lambda x: re.sub(r'[^ ㄱ - | 가-힣]+',\" \",x))\n",
    "data_df.head()\n"
   ],
   "id": "c7bbb220f0b3164"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 벡터화 작업\n",
    "data_title_tfidf = tfidf.transform(data_df[\"title\"])"
   ],
   "id": "3bb0d97743f3dc1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_title_predict = SA_lr_best.predict(data_title_tfidf)",
   "id": "e3f1b1c4486206c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.feature_extraction.text import TfidfVectorizer",
   "id": "885c78cddf452324"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 예측값을 새로운 컬럼으로 사용하기.\n",
    "data_df[\"title_label\"] = data_title_predict\n",
    "# description , 같은 작업 반복\n",
    "# 벡터화 작업\n",
    "data_description_tfidf = tfidf.transform(data_df[\"description\"])\n",
    "# 학습이 된 감성 분석 모델을 이용해서, 해당 제목으로, 긍정, 부정인지 평가하기.\n",
    "data_description_predict = SA_lr_best.predict(data_description_tfidf)\n",
    "# 예측값을 새로운 컬럼으로 사용하기.\n",
    "data_df[\"description_label\"] = data_description_predict\n",
    "data_df.head()\n",
    "# 파일 csv 로 옮기기\n",
    "data_df.to_csv(\"./\" + file_name + \".csv\", encoding=\"utf-8\")\n",
    "# 간단히, 해당 제목, 내용이 , 긍정, 부정의 글의 갯수가 몇개인지 파악해보기.\n",
    "print(data_df[\"title_label\"].value_counts())\n",
    "print(data_df[\"description_label\"].value_counts())\n",
    "# 제목, 내용 분리 작업.\n",
    "columns_name = ['title', 'title_label', 'description', 'description_label']\n",
    "NEG_data_df = pd.DataFrame(columns=columns_name)\n",
    "POS_data_df = pd.DataFrame(columns=columns_name)\n",
    "\n",
    "for i, data in data_df.iterrows():\n",
    "    title = data[\"title\"]\n",
    "    description = data[\"description\"]\n",
    "    t_label = data[\"title_label\"]\n",
    "    d_label = data[\"description_label\"]\n",
    "\n",
    "    if d_label == 0:  # 부정 감성 샘플만 추출\n",
    "        # NEG_data_df = NEG_data_df.append(pd.DataFrame([[title, t_label, description, d_label]],columns=columns_name),ignore_index=True)\n",
    "        new_data_df = pd.DataFrame([[title, t_label, description, d_label]], columns=columns_name)\n",
    "        NEG_data_df = pd.concat([NEG_data_df, new_data_df], ignore_index=True)\n",
    "    else:  # 긍정 감성 샘플만 추출\n",
    "        # POS_data_df = POS_data_df.append(pd.DataFrame([[title, t_label, description, d_label]],columns=columns_name),ignore_index=True)\n",
    "        new_data_df = pd.DataFrame([[title, t_label, description, d_label]], columns=columns_name)\n",
    "        POS_data_df = pd.concat([POS_data_df, new_data_df], ignore_index=True)\n",
    "# 파일에 저장.\n",
    "NEG_data_df.to_csv('./' + file_name + '_NES.csv', encoding='utf-8')\n",
    "POS_data_df.to_csv('./' + file_name + '_POS.csv', encoding='utf-8')\n",
    "len(NEG_data_df), len(POS_data_df)\n",
    "POS_description = POS_data_df['description']\n",
    "POS_description_noun_tk = []\n",
    "\n",
    "for d in POS_description:\n",
    "    POS_description_noun_tk.append(okt.nouns(d))  #형태소가 명사인 것만 추출\n",
    "print(POS_description_noun_tk)  #작업 확인용 출력\n",
    "POS_description_noun_join = []\n",
    "\n",
    "for d in POS_description_noun_tk:\n",
    "    d2 = [w for w in d if len(w) > 1]  #길이가 1인 토큰은 제외\n",
    "    POS_description_noun_join.append(\" \".join(d2))  # 토큰을 연결(join)하여 리스트 구성\n",
    "print(POS_description_noun_join)  #작업 확인용 출력\n",
    "NEG_description = NEG_data_df['description']\n",
    "\n",
    "NEG_description_noun_tk = []\n",
    "NEG_description_noun_join = []\n",
    "\n",
    "for d in NEG_description:\n",
    "    NEG_description_noun_tk.append(okt.nouns(d))  #형태소가 명사인 것만 추출\n",
    "\n",
    "for d in NEG_description_noun_tk:\n",
    "    d2 = [w for w in d if len(w) > 1]  #길이가 1인 토큰은 제외\n",
    "    NEG_description_noun_join.append(\" \".join(d2))  # 토큰을 연결(join)하여 리스트 구성\n",
    "print(NEG_description_noun_join)  #작업 확인용 출력\n",
    "POS_tfidf = TfidfVectorizer(tokenizer=okt_tokenizer, min_df=2)\n",
    "POS_dtm = POS_tfidf.fit_transform(POS_description_noun_join)\n",
    "POS_vocab = dict()\n",
    "\n",
    "for idx, word in enumerate(POS_tfidf.get_feature_names_out()):\n",
    "    POS_vocab[word] = POS_dtm.getcol(idx).sum()\n",
    "\n",
    "POS_words = sorted(POS_vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "POS_words  #작업 확인용 출력\n",
    "NEG_tfidf = TfidfVectorizer(tokenizer=okt_tokenizer, min_df=2)\n",
    "NEG_dtm = NEG_tfidf.fit_transform(NEG_description_noun_join)\n",
    "NEG_vocab = dict()\n",
    "\n",
    "for idx, word in enumerate(NEG_tfidf.get_feature_names_out()):\n",
    "    NEG_vocab[word] = NEG_dtm.getcol(idx).sum()\n",
    "\n",
    "NEG_words = sorted(NEG_vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "NEG_words  #작업 확인용 출력\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fm._get_fontconfig_fonts()\n",
    "font_location = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_location).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "\n",
    "max = 15  #바 차트에 나타낼 단어의 수\n",
    "plt.bar(range(max), [i[1] for i in POS_words[:max]], color=\"blue\")\n",
    "plt.title(\"긍정 뉴스의 단어 상위 %d개\" % max, fontsize=15)\n",
    "plt.xlabel(\"단어\", fontsize=12)\n",
    "plt.ylabel(\"TF-IDF의 합\", fontsize=12)\n",
    "plt.xticks(range(max), [i[0] for i in POS_words[:max]], rotation=70)\n",
    "\n",
    "plt.show()\n",
    "plt.bar(range(max), [i[1] for i in NEG_words[:max]], color=\"red\")\n",
    "plt.title(\"부정 뉴스의 단어 상위 %d개\" % max, fontsize=15)\n",
    "plt.xlabel(\"단어\", fontsize=12)\n",
    "plt.ylabel(\"TF-IDF의 합\", fontsize=12)\n",
    "plt.xticks(range(max), [i[0] for i in NEG_words[:max]], rotation=70)\n",
    "\n",
    "plt.show()"
   ],
   "id": "2e343d164254f428"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "description = data_df['description']\n",
    "description_noun_tk = []\n",
    "\n",
    "for d in description:\n",
    "    description_noun_tk.append(okt.nouns(d))  #형태소가 명사인 것만 추출\n",
    "\n",
    "description_noun_tk2 = []\n",
    "\n",
    "for d in description_noun_tk:\n",
    "    item = [i for i in d if len(i) > 1]  #토큰의 길이가 1보다 큰 것만 추출\n",
    "    description_noun_tk2.append(item)\n",
    "dictionary = corpora.Dictionary(description_noun_tk2)\n",
    "print(dictionary[1])  #작업 확인용 출력\n",
    "corpus = [dictionary.doc2bow(word) for word in description_noun_tk2]\n",
    "print(corpus)  #작업 확인용 출력\n",
    "# ✔️ LDA는 문서에서 숨겨진 토픽을 찾아내는 알고리즘\n",
    "# ✔️ 토픽을 정의할 필요 없이, 문서에서 자동으로 토픽을 추출\n",
    "# ✔️ 토픽마다 특정 단어들이 높은 확률로 등장\n",
    "# ✔️ 각 문서가 어떤 토픽에 속하는지 분석 가능\n",
    "k = 4  #토픽의 개수 설정\n",
    "lda_model = gensim.models.ldamulticore.LdaMulticore(corpus, iterations=12, num_topics=k, id2word=dictionary, passes=1,\n",
    "                                                    workers=10)\n",
    "# corpus: 문서를 토큰화하여 BOW(Bag-of-Words) 형식으로 변환한 데이터\n",
    "# iterations=12: 각 문서에서 토픽을 업데이트하는 반복 횟수\n",
    "# num_topics=k: 찾을 토픽 개수 (여기서는 4개)\n",
    "# id2word=dictionary: 단어 ID와 단어 매핑 정보\n",
    "# passes=1: 전체 데이터셋을 몇 번 학습할지 (값이 클수록 학습이 더 많이 됨)\n",
    "# workers=10: 병렬처리를 위한 CPU 코어 개수\n",
    "\n",
    "print(lda_model.print_topics(num_topics=k, num_words=15))\n",
    "#최초 한번만 설치\n",
    "# !pip install pyLDAvis\n",
    "# !pip install --upgrade pandas\n",
    "import pickle\n",
    "\n",
    "with open(\"lda_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lda_model, f)\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "# lda_vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, n_jobs=1)"
   ],
   "id": "ab56fa1ace2aac24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pyLDAvis.display(lda_vis)\n",
   "id": "3ff161573e461c80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
